{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b><font size=6>Machine Learning for Networks<b><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><b><font size=4>SSH Shell Attack session<b><left>                                                                   \n",
    "##### Group 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Models \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "\n",
    "#Language Models Exploration \n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense  \n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=4>Section 1 – Data exploration and pre-processing<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset with all the SSH sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df_original = pd.read_parquet('ssh_attacks.parquet')\n",
    "df=df_original.copy()\n",
    "# Convert 'first_timestamp' column to datetime objects\n",
    "df['first_timestamp'] = pd.to_datetime(df['first_timestamp'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. When are the attacks performed? Analyze the temporal series.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new DataFrame containing only attack instances\n",
    "# Excluding sessions labeled as \"Harmless\" with a single label\n",
    "df_attacks = df.loc[~df[\"Set_Fingerprint\"].apply(lambda x : \"Harmless\" in x and len(x) == 1)]\n",
    "\n",
    "# Count the number of attacks per day\n",
    "# Extract the date from 'first_timestamp', count occurrences, sort by date\n",
    "attacks_per_day = df_attacks['first_timestamp'].dt.date.value_counts().sort_index().to_frame(\"Number_of_attacks_per_day\")\n",
    "\n",
    "# Display the resulting DataFrame showing the number of attacks per day\n",
    "attacks_per_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of attack occurrences based on the temporal series of first_timestamp showcases a trend in attack frequency over time.\n",
    "The dataset has been transformed to interpret the first_timestamp column as datetime objects for accurate temporal analysis. The subsequent process isolates attack instances within the dataset, excluding records tagged as \"Harmless\" with a single label.\n",
    "The resulting analysis presents the number of attacks per day:\n",
    "\n",
    "| Date_time | Attacks |\n",
    "| --- | --- |\n",
    "|June 4th, 2019 | 82 attacks |\n",
    "|June 5th, 2019 | 124 attacks |\n",
    "|June 6th, 2019 | 117 attacks |\n",
    "|June 7th, 2019 | 121 attacks |\n",
    "|June 8th, 2019 | 118 attacks |\n",
    "| ... (continues with dates up to) |\n",
    "| February 25th, 2020| 649 attacks |\n",
    "| February 26th, 2020| 483 attacks |\n",
    "| February 27th, 2020| 551 attacks |\n",
    "| February 28th, 2020| 580 attacks |\n",
    "| February 29th, 2020| 627 attacks |\n",
    "\n",
    "This temporal series reveals fluctuations in attack intensity over time, with notable spikes and drops in attack occurrences. The observations suggest potential patterns or trends that could be further explored to understand the dynamics of these SSH shell attacks across different periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size for the plot\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Create a line plot using seaborn\n",
    "sns.lineplot(\n",
    "    data = attacks_per_day,          # Use the 'attacks_per_day' DataFrame for plotting\n",
    "    x = attacks_per_day.index,       # X-axis represents the timestamp of attacks\n",
    "    y = \"Number_of_attacks_per_day\", # Y-axis represents the number of attacks per day\n",
    "    marker='o',                      # Marker style for data points\n",
    "    linestyle='-',                   # Style of the line connecting the data points\n",
    "    color= 'blue',                   # Color of the line\n",
    "    markersize=5                     # Size of markers\n",
    ")\n",
    "\n",
    "# Set plot title and labels for axes\n",
    "plt.title('Number of Attacks')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of attacks\")\n",
    "\n",
    "# Format the date on the x-axis to display day-month-year\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%y'))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Ensure proper layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization depicting attack frequencies over time reveals distinct patterns:\n",
    "\n",
    "Between June 2019 and September 2019, there is a conspicuous decrease in attack occurrences, indicating a phase of minimal activity. Subsequently, there is a significant surge in attack instances towards the latter part of 2019, signifying a notable rise in both the frequency and intensity of attacks during this period.\n",
    "\n",
    "This timeline underscores a stark contrast between the relatively quiet phase observed from June to September 2019 and the pronounced escalation in attack activities, particularly notable in the latter months of the year. This shift in trend emphasizes a substantial alteration in attack behavior, marked by an extended period of low activity succeeded by a considerable surge in attack incidents towards the year's end.\n",
    "\n",
    "Moreover, at the beginning of 2020, there is a noticeable decline in attack occurrences once more. This decline follows the heightened activity observed in late 2019, representing a shift from the increased attack rates back to a decreased frequency as the year transitions into its initial months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code aggregates attack instances per year, creating a DataFrame named attacks_per_year\n",
    "\n",
    "# Group attack instances by year and count occurrences\n",
    "attacks_per_year = df_attacks.groupby(df['first_timestamp'].dt.year).size().to_frame(\"Number_attacks\").reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "attacks_per_year.rename(columns={\"first_timestamp\": \"Year\"}, inplace=True)\n",
    "attacks_per_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contrast in attack counts between the two years, with 2019 showing a significantly higher number of attacks compared to 2020, can be reasonably attributed to the limited temporal coverage of the dataset for the year 2020. With data available for only two months of 2020, the reduced number of observations in this period is expected and explains the lower count of attacks for that year compared to the extensive records available for 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size for the plot\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "# Set the figure size for the plot\n",
    "sns.barplot(\n",
    "    data = attacks_per_year,   # Use the 'attacks_per_year' DataFrame for plotting\n",
    "    x = \"Year\",                # X-axis represents the years (2019 and 2020)\n",
    "    y = \"Number_attacks\",      # Y-axis represents the number of attacks\n",
    "    hue= \"Year\",\n",
    "    palette = ['blue','orange']\n",
    ")\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.ylabel(\"Number of attacks\")\n",
    "plt.title(\"Number of total attacks in 2019 and 2020\")\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to study the trend of the number of attacks during the time range considered, would be usefull to analyse data an year at time, as the the chart below is showing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter attack instances for the year 2019\n",
    "df_attacks_2019 = df_attacks.loc[df_attacks[\"first_timestamp\"].dt.year == 2019]\n",
    "# Group attacks by month and count occurrences\n",
    "df_attacks_2019_month = df_attacks_2019.groupby(df_attacks_2019[\"first_timestamp\"].dt.month).size().to_frame(\"Number_attacks_2019_month\").reset_index()\n",
    "\n",
    "# Filter attack instances for the year 2020\n",
    "df_attacks_2020 = df_attacks.loc[df_attacks[\"first_timestamp\"].dt.year == 2020]\n",
    "# Group attacks by month and count occurrences\n",
    "df_attacks_2020_month = df_attacks_2020.groupby(df_attacks_2020[\"first_timestamp\"].dt.month).size().to_frame(\"Number_attacks_2020_month\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a figure with two subplots\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Plot for 2019\n",
    "plt.subplot(1,2,1)\n",
    "sns.barplot(\n",
    "    data = df_attacks_2019_month,\n",
    "    x = \"first_timestamp\",\n",
    "    y = \"Number_attacks_2019_month\",\n",
    "    color = 'blue'  \n",
    ")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of attacks\")\n",
    "plt.title(\"2019\")\n",
    "\n",
    "# Plot for 2020\n",
    "plt.subplot(1,2,2)\n",
    "sns.barplot(\n",
    "    data = df_attacks_2020_month,\n",
    "    x = \"first_timestamp\",\n",
    "    y = \"Number_attacks_2020_month\",\n",
    "    color = 'orange'\n",
    ")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of attacks\")\n",
    "plt.title(\"2020\")\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout(pad = 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar charts reveal intriguing trends:\n",
    "\n",
    "- In 2019, there is a noticeable surge in the number of attacks from months 9 to 12. A progressive increase in attack activity is observed during these months, reaching a peak towards the year's end.\n",
    "\n",
    "- At the onset of 2020, in months 1 and 2, a relatively similar frequency of attacks is noted, approximately representing half the number of attacks compared to month 10 in 2019.\n",
    "\n",
    "These patterns outline a significant uptick in attack activity towards the end of 2019, followed by a comparatively steady beginning in 2020 with a considerably lower number of attacks compared to the peak period of the previous year.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we worked to the whole dataset of sessions. But there are a part of them that bring to harmless attaks. So sessions that are been analysed by the honeypot that was collecting the shell attacks, but finally they're been classified as harmless.\n",
    "\n",
    "Let's have a look to the temporal distribution of these harmless sessiosns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering harmless sessions based on 'Set_Fingerprint' column\n",
    "df_harmless = df[df[\"Set_Fingerprint\"].apply(lambda x : \"Harmless\" in x and len(x) == 1)]\n",
    "\n",
    "# Counting harmless sessions per day\n",
    "harmless_per_day = df_harmless['first_timestamp'].dt.date.value_counts().sort_index().to_frame(\"Number_harmless_per_day\")\n",
    "\n",
    "# Creating a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.barplot(\n",
    "    data = harmless_per_day,\n",
    "    x = harmless_per_day.index, \n",
    "    y = \"Number_harmless_per_day\",\n",
    "    hue = harmless_per_day.index,\n",
    "    legend = False,\n",
    "    palette = \"icefire\"\n",
    ")\n",
    "plt.title('Temporal Distribution of Harmless Sessions')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Harmless Sessions')\n",
    "plt.xticks(rotation = 45, fontsize = 4)  # Rotate x-axis labels by 45 degrees\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can't see a particular distribution, or periods with important differences in terms of the number of harmless attacks.\n",
    "These harmless attacks can be errors in the scanning tools or false positive.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2. Extract features from the attack sessions. How does the empirical distribution of the number of\n",
    "characters in each session look like? How is the distribution of the number of word per session?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_session(full_session):\n",
    "    return [word for word in list(filter(None, re.split(\";|/|-|\\||\\.|=|$| \", full_session))) if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another dataframe to show the number of characters and words in each session\n",
    "# Number_characters: Calculates the number of alphanumeric characters in each session.\n",
    "number_characters = df['full_session'].apply(lambda x: len([char for char in x if char.isalpha()]))\n",
    "\n",
    "# Number_words: Computes the number of words (considering only alphanumeric characters) in each session.\n",
    "# the regex splits the bash command with multiple delimiters (;, /, -, |,  ,)\n",
    "number_words = df['full_session'].apply(lambda x: len(clean_session(x)))\n",
    "\n",
    "data = {\"number_characters\": number_characters, \"number_words\": number_words}\n",
    "df_number_characters_words = pd.DataFrame(data = data)\n",
    "df_number_characters_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code creates two side-by-side histograms:\n",
    "# Histogram of Character Counts per Session: Displays the distribution of the number of characters in each session. It bins the data into 50 bins.\n",
    "# Histogram of Word Counts per Session: Shows the distribution of the number of words in each session. It also bins the data into 50 bins.\n",
    "# These histograms help visualize the distributions of character and word counts within the attack sessions, providing insights into the length and complexity of these sessions in terms of characters and words. \n",
    "\n",
    "# Tracking histograms for character and word counts per session.\n",
    "plt.figure(figsize=(11, 5))\n",
    "\n",
    "# Histogram for the number of characters per session.\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.hist(df_number_characters_words['number_characters'], bins=50, color = 'blue')\n",
    "plt.title('Distribution of the number of characters per session')\n",
    "plt.xlabel('Number of Characters')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "\n",
    "# Histogram for the number of words per session\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_number_characters_words['number_words'], bins = 50 , color = 'orange') \n",
    "plt.title('Distribution of the number of words per session')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, it's possible to see that the distribution of words per session and the distribution of characters per session predominantly concentrate below 2000 words and below 20,000 characters, respectively.\n",
    "\n",
    "To have a clearer idea of the distribution, we decided to limit the x-axis for both plots so that we can closely examine these two distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking histograms for character and word counts per session.\n",
    "plt.figure(figsize=(11, 5))\n",
    "\n",
    "# Histogram for the number of characters per session.\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.hist(df_number_characters_words['number_characters'], bins=50, range=(0, 700), color = 'blue')\n",
    "plt.title('Distribution of the number of characters per session')\n",
    "plt.xlabel('Number of Characters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0, 700)\n",
    "\n",
    "# Histogram for the number of words per session\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_number_characters_words['number_words'], bins = 50, range = (0, 70) , color = 'orange')\n",
    "plt.title('Distribution of the number of words per session')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0, 70)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histograms, it is observed that the distribution of character counts per session centers is concentrated around approximately 350 characters.\n",
    "Moreover, regarding the number of words per session, the distribution frequently peaks around 40 to 48 words. This indicates that sessions often contain this range of word counts, emphasizing a typical occurrence of sessions with this word count range. These insights provide a clear understanding of the common lengths observed within the attack sessions, both in terms of characters and words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3. What are the most common words in the sessions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Joining all text from 'full_session' into a single string\n",
    "full_session = ' '.join(df['full_session'])\n",
    "\n",
    "# Cleaning the text, leaving only alpha numeric words\n",
    "session_cleaned = clean_session(full_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each word and extract the top 10 most common words\n",
    "df_list_session_words = pd.Series(session_cleaned).value_counts().head(10)\n",
    "\n",
    "# Identify the most common word and its frequency\n",
    "most_common_word = df_list_session_words.idxmax()    # Most common word\n",
    "frequency = df_list_session_words.max()              # Frequency of the most common word\n",
    "\n",
    "# Convert the Series of word frequencies to a dictionary\n",
    "word_freq=df_list_session_words.to_dict()\n",
    "most_common_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common word is : 'tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary that holds the frequencies of the top 10 most common words\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "# Display the WordCloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This representation emphasizes words based on their frequency in the sessions. Words that appear more frequently will be displayed larger and more prominently within the WordCloud. The interpolation='bilinear' argument enhances the image quality for better clarity. The plt.axis('off') command removes the axis for a cleaner visual appearance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that many common linux command are used in the attack sessions. \n",
    "- \"**grep**\", the most frequent of the set, can be useful for instance to find some text pattern in files, or to find some specific file/directory in the system. \n",
    "\n",
    "- \"**Cat**\" is used to display content of file, that's what an attacker shoud want, to look at some sensitive data, private data or configuration files for example.\n",
    "\n",
    "- \"**Rm**\" could indicate an attempt to remove files/directories. \n",
    "\n",
    "And so on...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4. How are the intents distributed? How many intents per session do you observe? What are the most common intents? How are the intents distributed in time?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting relevant columns: session_id and Set_Fingerprint\n",
    "intents_df = df[[\"session_id\", \"Set_Fingerprint\"]]\n",
    "\n",
    "# Expanding the 'Set_Fingerprint' column to individual intents and sessions\n",
    "intents_df_exploted = intents_df.explode('Set_Fingerprint')\n",
    "\n",
    "# Grouping by session and counting the number of intents per session\n",
    "intents_df_grouped = intents_df_exploted.groupby(\"session_id\").size()\n",
    "\n",
    "# Plotting the distribution of intents per session\n",
    "\n",
    "#TODO SHOULD WE REMOVE THIS PLOT ?\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.kdeplot(\n",
    "    data = intents_df_grouped\n",
    ")\n",
    "plt.xlabel(\"Sessions\")\n",
    "plt.ylabel(\"Number of intents\")\n",
    "plt.title(\"Number of Intents per Sessions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that many sessions have differents intents. Obviously an attacker, once inside the server, probably wants to damage the system in differents ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = intents_df_exploted.groupby('Set_Fingerprint').count().sort_values(by='session_id', ascending=False)\n",
    "\n",
    "# Distribution plot of intents \n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Generate the figures\n",
    "plt.figure(figsize=(6,3))\n",
    "sns.barplot(intents, x='session_id', y=intents.index, color='blue', hue_order=intents.index)\n",
    "plt.title('Distribution of Intents')\n",
    "plt.xlabel('Number of intents')\n",
    "plt.ylabel('Type of Intents')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presented bar chart shows the most common intentions found in the dataset; Discovery, Persistence and Execution lead the most used type of attacks for each of the sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO REVIEW THIS PLOT\n",
    "\n",
    "# Explode the 'Set_Fingerprint' column and group by timestamp and intents, filling missing values with 0\n",
    "df_grouped = df.explode('Set_Fingerprint').groupby([pd.Grouper(key='first_timestamp', freq='D'), 'Set_Fingerprint']).size().unstack().fillna(0)\n",
    "\n",
    "# Plotting the distribution of intents over time\n",
    "df_grouped.plot(\n",
    "    figsize = (10,6), \n",
    "    xlabel = \"Date\", \n",
    "    ylabel = \"Distribution of the Intents\", \n",
    "    title = \"Distribution of the Intents in Time\")\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of intents unfolds as follows:\n",
    "\n",
    "**Defense Evasion**:\n",
    "No significant peaks noticed, with a few attacks observed between July and September 2019, remaining consistently low alongside 'Harmless', 'Impact', and 'Other' intents.\n",
    "\n",
    "**Execution**:\n",
    "Displays a sharp peak towards the end of 2019, notably in the last two months.\n",
    "\n",
    "**Persistence and Discovery**:\n",
    "Showcase an intriguing trend, reaching their highest peaks towards the end of 2019. These intents exhibit the highest frequency, notably surging towards the year-end, reaching maximum levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5. How can text represented numerically? Try to convert the text into numerical representations\n",
    "(vectors) through Bag of Words (BoW)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_session_cleaned = df.copy()\n",
    "df_session_cleaned.update(df_session_cleaned[\"full_session\"].apply(lambda x : str(clean_session(x))))\n",
    "df_session_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df = 0.05)\n",
    "bow = count_vectorizer.fit_transform(df_session_cleaned[\"full_session\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_session_cleaned_bow = pd.DataFrame(bow.toarray(), index=df_session_cleaned.index, columns = list(count_vectorizer.vocabulary_.keys()))\n",
    "df_session_cleaned_bow = pd.concat([df_session_cleaned, df_session_cleaned_bow], axis=1)\n",
    "df_session_cleaned_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.6. Associate each word in each attack session with its TF-IDF value (Term Frequency-Inverse Document Frequency)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a method to evaluate the importance of words in a document. \n",
    "\n",
    "It can do it measuring how often a word appears in a document and, if we have many documents, the frequency along all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df = 0.05)\n",
    "tfid = tfidf_vectorizer.fit_transform(df_session_cleaned[\"full_session\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_session_cleaned_tfidf = pd.DataFrame(tfid.toarray(), index=df_session_cleaned.index, columns = list(tfidf_vectorizer.vocabulary_.keys()))\n",
    "df_session_cleaned_tfidf = pd.concat([df_session_cleaned, df_session_cleaned_tfidf], axis=1)\n",
    "df_session_cleaned_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once applied TF-IDF, we have a new dataframe with 61 new features corresponding to the differents words used in the session. \n",
    "But 61 is to much for us, will require an heavy computational power, and so much time to train the models.\n",
    "So we want to reduce the number fo features. To do that we firstly compute the correlation matrix of only the 61 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Correlation Matrix</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "df_features_bow = df_session_cleaned_bow.drop(columns=[\"session_id\", \"full_session\", \"first_timestamp\", \"Set_Fingerprint\"])\n",
    "correlation_matrix_bow = df_features_bow.corr().abs()\n",
    "\n",
    "# Compute the heatmap of the correlation matrix}\n",
    "plt.figure(figsize=(50,50))\n",
    "sns.heatmap(correlation_matrix_bow, cmap='Blues', annot=True, vmin=.0, vmax=1, cbar_kws={'label':'Correlation'})\n",
    "plt.title('Correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features having a correlation (with a covariance) > 0.98\n",
    "c = correlation_matrix_bow[correlation_matrix_bow > 0.8]\n",
    "s = c.unstack()\n",
    "so = s.sort_values(ascending=False).reset_index()\n",
    "\n",
    "# Get strongly correlatead features, removing pairs having correlation = 1 because of the diagonal,\n",
    "# i.e., correlation between one feature and itself\"\n",
    "so = so[(so[0].isnull()==False) & (so[\"level_0\"] != so[\"level_1\"])]\n",
    "to_be_deleted = []\n",
    "candidates = list(so[\"level_0\"])\n",
    "\n",
    "# Get the unique set of features to be deleted\n",
    "# Notice that we discard one feature per time considering the case where a feature is strongly correlated\n",
    "# with multiple features\n",
    "subset_so = so\n",
    "for candidate in candidates:\n",
    "    if (candidate in list(subset_so[\"level_0\"])): \n",
    "        to_be_deleted.append(candidate) # add the feature to the removed candidates\n",
    "        # remove the rows where the removed feature is involved\n",
    "        subset_so = subset_so[(subset_so[\"level_0\"] != candidate) & (subset_so[\"level_1\"] != candidate)] \n",
    "print(len(to_be_deleted), 'features to be removed')\n",
    "to_be_deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "df_features_tfidf = df_session_cleaned_tfidf.drop(columns=[\"session_id\", \"full_session\", \"first_timestamp\", \"Set_Fingerprint\"])\n",
    "correlation_matrix_tfidf = df_features_tfidf.corr().abs()\n",
    "\n",
    "# Compute the heatmap of the correlation matrix}\n",
    "plt.figure(figsize=(50,50))\n",
    "sns.heatmap(correlation_matrix_tfidf, cmap='Blues', annot=True, vmin=.0, vmax=1, cbar_kws={'label':'Correlation'})\n",
    "plt.title('Correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features having a correlation (with a covariance) > 0.98\n",
    "c = correlation_matrix_tfidf[correlation_matrix_tfidf > 0.8]\n",
    "s = c.unstack()\n",
    "so = s.sort_values(ascending=False).reset_index()\n",
    "\n",
    "# Get strongly correlatead features, removing pairs having correlation = 1 because of the diagonal,\n",
    "# i.e., correlation between one feature and itself\"\n",
    "so = so[(so[0].isnull()==False) & (so[\"level_0\"] != so[\"level_1\"])]\n",
    "to_be_deleted = []\n",
    "candidates = list(so[\"level_0\"])\n",
    "\n",
    "# Get the unique set of features to be deleted\n",
    "# Notice that we discard one feature per time considering the case where a feature is strongly correlated\n",
    "# with multiple features\n",
    "subset_so = so\n",
    "for candidate in candidates:\n",
    "    if (candidate in list(subset_so[\"level_0\"])): \n",
    "        to_be_deleted.append(candidate) # add the feature to the removed candidates\n",
    "        # remove the rows where the removed feature is involved\n",
    "        subset_so = subset_so[(subset_so[\"level_0\"] != candidate) & (subset_so[\"level_1\"] != candidate)] \n",
    "print(len(to_be_deleted), 'features to be removed')\n",
    "to_be_deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_bow = PCA()\n",
    "pca_bow.fit(df_features_bow)\n",
    "explained_variance_bow = pca_bow.explained_variance_ratio_\n",
    "cumul_exp_var_bow = np.cumsum(explained_variance_bow)\n",
    "perc_cumul_exp_var_bow = cumul_exp_var_bow * 100\n",
    "\n",
    "circle = plt.Circle((10, 99.9), 0.5, color=\"red\", fill=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.lineplot(\n",
    "    data = perc_cumul_exp_var_bow,\n",
    "    marker=\"o\"\n",
    ")\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.add_patch(circle)\n",
    "plt.xlabel(\"Principal Components\")\n",
    "plt.ylabel(\"Cumulative explained variance [%]\")\n",
    "plt.grid()\n",
    "plt.title(\"PCA\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_bow = PCA(n_components=10)\n",
    "pca_result_bow = pca_bow.fit_transform(df_features_bow)\n",
    "pca_result_bow = pd.DataFrame(pca_result_bow, columns=[f'PC{i}' for i in range(pca_bow.n_components_)])\n",
    "pca_result_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_pca_bow = pca_result_bow.corr().abs()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(correlation_matrix_pca_bow, cmap='Blues', annot=True, vmin=.0, vmax=1, cbar_kws={'label':'Correlation'}, fmt='.2f')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_tfidf = PCA()\n",
    "pca_tfidf.fit(df_features_tfidf)\n",
    "explained_variance_tfidf = pca_tfidf.explained_variance_ratio_\n",
    "cumul_exp_var_tfidf = np.cumsum(explained_variance_tfidf)\n",
    "perc_cumul_exp_var_tfidf = cumul_exp_var_tfidf * 100\n",
    "\n",
    "circle = plt.Circle((20,99.7), 0.5, color=\"red\", fill=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.lineplot(\n",
    "    data = perc_cumul_exp_var_tfidf,\n",
    "    marker=\"o\"\n",
    ")\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.add_patch(circle)\n",
    "plt.xlabel(\"Principal Components\")\n",
    "plt.ylabel(\"Cumulative explained variance [%]\")\n",
    "plt.grid()\n",
    "plt.title(\"PCA\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_tfidf = PCA(n_components=20)\n",
    "pca_result_tfidf = pca_tfidf.fit_transform(df_features_tfidf)\n",
    "pca_result_tfidf = pd.DataFrame(pca_result_tfidf, columns=[f'PC{i}' for i in range(pca_tfidf.n_components_)])\n",
    "pca_result_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_pca_tfidf = pca_result_tfidf.corr().abs()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(correlation_matrix_pca_tfidf, cmap='Blues', annot=True, vmin=.0, vmax=1, cbar_kws={'label':'Correlation'}, fmt='.2f')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE I STOPPED HERE TO REVIEW THE PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_features.drop(columns=to_be_deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The final DataFrame has :', df_features.shape[1], 'features.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_corr = pd.concat([df_session_cleaned_bow[[\"session_id\", \"full_session\",\"first_timestamp\",\"Set_Fingerprint\"]], df_features], axis=1)\n",
    "result_df_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=4>Section 2 – Supervised Learning – Classification<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"text-align: justify\"> Classify the tactics of an attack session, based on the used words in the text and also possibly on time. Notice that each session have multiple labels. Hence you can decompose the problem into multiple binary classification problems. For each attack session, you have to solve the 7 binary classification problem, one for each possible label {'Persistence', 'Discovery', 'Defense Evasion', 'Execution', 'Impact', 'Other', 'Harmless'}. </div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Perform a split to segment the dataset into training and test dataset. If you want to standardize your dataset, fit the scaler on training set and transforming both training and test. Notice that the sklearn implementation of tf-idf already performs the standardization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = df_features.columns\n",
    "\n",
    "X_feature = result_df_corr.filter(features_names)\n",
    "y_feature =  result_df_corr[\"Set_Fingerprint\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_feature,\n",
    "    y_feature,\n",
    "    train_size = 0.7,             # 70% of the data is for trainning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The size of trainning set is:', len(X_train))\n",
    "print('The size of test set is:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardization of the Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the TF-IDF pre-processing was applied previously to all the sessions, the data considered as features was already standardized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardization of the Categorical Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Use MultiLabelBinarizer to transform the labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_mlb = mlb.fit_transform(y_train)\n",
    "y_test_mlb = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Used Preprocessing Techniques** \n",
    "<br>\n",
    "<div style=\"text-align: justify\"> A <b>MultiLabelBinarizer</b> is a transformer that is used for multi-label classification problems, in order to handle the cases where each sample belongs to multiple classes simultaneously. The purpose of MultiLabelBinarizer is to convert a collection of sequences of labels into a binary matrix format. The binary classification of each label in the 'Set_Fingerprint' column was performed by converting the multi-class label matrix into a binary matrix, where each column represents one of the possible classes and each row represents one instance. </div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> <b>TF-IDF </b> The TF-IDF value helps to identify words that are both frequent within a document and distinctive across the entire corpus. This dual consideration allows TF-IDF to highlight terms that are potentially more meaningful for distinguishing documents in tasks such as text classification, information retrieval, and document similarity analysis. </div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> MultiLabelBinarizer is used to handle categorical variables before fitting a model, as most machine learning algorithms can only handle numerical data.</div><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Choose at least 2 ML methods, and perform the model training, with default parameter\n",
    "configuration, evaluating the performance on both training and test set. Output the confusion\n",
    "matrix and classification report. Do you observe overfitting or under-fitting? Which model\n",
    "generates the best performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><left><b><font size=4> Random Forest (RF)<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">Random Forest (RF) serves as a classification model that constructs a collection of decision trees (DT) using a randomly chosen subset of the given training set. The model aggregates the individual decisions made by each decision tree and combines their votes to make the ultimate prediction.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(n_estimators=100) #(n_estimators=30, max_depth=50)\n",
    "\n",
    "st = time.time()\n",
    "# Trainning the model\n",
    "model_rf.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print(r'Time to train the model:', elapsed_time,'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predictions on training set\n",
    "y_train_predictions = model_rf.predict(X_train)\n",
    "\n",
    "# Predictions on test set\n",
    "y_test_predictions = model_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "accu = model_rf.score(X_train, y_train_mlb)\n",
    "accuracy = metrics.accuracy_score(y_train_mlb, y_train_predictions)\n",
    "print(f\"Accuracy of the 'Random Forest' model for the trainning set: {accuracy:.2f}, {accu:.2f}\")\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "accu = model_rf.score(X_test, y_test_mlb)\n",
    "accuracy = metrics.accuracy_score(y_test_mlb, y_test_predictions)\n",
    "print(f\"Accuracy of the 'Random Forest' model for test set: {accuracy:.2f}, {accu:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=3 >Classification Report<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Evaluate performance on training set\n",
    "report_trainning = classification_report(y_train_mlb, y_train_predictions, target_names=mlb.classes_, output_dict=True)\n",
    "df_report_trainning = pd.DataFrame(report_trainning).transpose()\n",
    "df_report_trainning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Set (Test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on test set\n",
    "report_test = classification_report(y_test_mlb, y_test_predictions, target_names=mlb.classes_, output_dict=True)\n",
    "df_report_test = pd.DataFrame(report_test).transpose()\n",
    "df_report_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=3> Confusion Matrix <b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_rf_train = multilabel_confusion_matrix(y_train_mlb, y_train_predictions)\n",
    "\n",
    "# Printing the confusion matrix\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    print(f\"Confusion Matrix for '{label}':\")\n",
    "    print(confusion_rf_train[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.title(f\"Confusion Matrix for '{label}'\")\n",
    "    plt.imshow(confusion_rf_train[i], cmap='Blues', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(np.arange(2), ['Negative', 'Positive'])\n",
    "    plt.yticks(np.arange(2), ['Negative', 'Positive'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_rf_test = multilabel_confusion_matrix(y_test_mlb, y_test_predictions)\n",
    "\n",
    "# Printing the confusion matrix\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    print(f\"Confusion Matrix for '{label}':\")\n",
    "    print(confusion_rf_test[i])\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.title(f\"Confusion Matrix for '{label}' in Test Set\")\n",
    "    plt.imshow(confusion_rf_test[i], cmap='Oranges', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(np.arange(2), ['Negative', 'Positive'])\n",
    "    plt.yticks(np.arange(2), ['Negative', 'Positive'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=4>K-Nearest Neighbors (KNN)<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">K-Nearest Neighbors (KNN) operates as a supervised learning classifier that relies on the concept of proximity to perform classifications or predictions for individual data points. Its fundamental principle is grounded in the notion that similar data points tend to cluster together. In the context of classification tasks, KNN assigns a class label to a data point by considering the majority vote of its nearest neighbors. Put simply, it selects the label that is most prevalent among the neighboring data points in close proximity to the one being evaluated.</div><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_mlb\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the k-NN model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "st = time.time()\n",
    "# Train the model on the training data\n",
    "knn.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print(r'Time to train the model:', elapsed_time,'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set\n",
    "predictions_train = knn.predict(X_train) \n",
    "\n",
    "# Generate predictions on the test set\n",
    "predictions_test = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the trainning data\n",
    "accuracy = knn.score(X_test, y_test_mlb)\n",
    "print(f\"Accuracy of the k-NN model for the trainning set: {accuracy:.2f}\")\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "accuracy = knn.score(X_test, y_test_mlb)\n",
    "print(f\"Accuracy of the k-NN model for the test set: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=3 >Classification Report<b><left>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report for trainning set\n",
    "report_train_knn = classification_report(y_train_mlb, predictions_train, target_names=mlb.classes_, output_dict=True)\n",
    "df_report_train = pd.DataFrame(report_train_knn).transpose()\n",
    "print(\"Classification Report for Trainning set:\")\n",
    "df_report_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report for test set\n",
    "report_test_knn = classification_report(y_test_mlb, predictions_test, target_names=mlb.classes_, output_dict=True)\n",
    "df_report_test_knn = pd.DataFrame(report_test_knn).transpose()\n",
    "print(\"Classification Report for Test set:\")\n",
    "df_report_test_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=3> Confusion Matrix <b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code presented below prints a series of confusion matrices for each class, displaying True-Positive (top-left), False-Negative (bottom-left), False-Positive (top-right), and True-Negative (bottom-right) counts.\n",
    "- True Positives (TP): Predicted correctly as positive.\n",
    "- False Positives (FP): Predicted as positive but actually negative.\n",
    "- False Negatives (FN): Predicted as negative but actually positive.\n",
    "- True Negatives (TN): Predicted correctly as negative.\n",
    "\n",
    "Each value in the confusion matrix represents the count of instances falling into these categories for a specific label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with a confusion matrix and classification report\n",
    "confusion_knn_test = multilabel_confusion_matrix(y_test_mlb, predictions_test)\n",
    "# Printing the confusion matrix\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    print(f\"Confusion Matrix for {label}:\")\n",
    "    print(confusion_knn_test[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.title(f\"Confusion Matrix for '{label}'\")\n",
    "    plt.imshow(confusion_knn_test[i], cmap='Oranges', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(np.arange(2), ['Negative', 'Positive'])\n",
    "    plt.yticks(np.arange(2), ['Negative', 'Positive'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> To know whether the model is underfitting or overfitting, we must first define these two terms. <br>\n",
    "<div style=\"text-align: justify\"><br><b>Underfitting</b> occurs when a model is too simple to capture the underlying patterns in the training data, resulting in poor performance even on the training set.  One indicator used to identify this modelling error is to look at the results; if both training and validation/testing performance are poor, the model is considered to be underfitted.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"><b>Overfitting</b> occurs when a model not only learns the underlying patterns in the training data, but also captures noise and random fluctuations, causing it to perform poorly on new and unknown data. One indicator used to identify this modelling error is to look at the results, if the model performs well on training data but poorly on validation or test data, it is considered overfitted.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">Once these two terms have been defined, it is possible to move on to the results obtained for both models. It is important to mention that, for both tests performed with the different classifiers, the default parameters were used.</div><br>\n",
    "<div style=\"text-align: justify\">For the <b>Random Forest (RF)</b> case, the default parameter implied the number of estimators equal to 100, and the tree depth was set to <i>'None'</i>. On the other hand, for the <b>K-Nearest Neighbor (KNN)</b> classifier, the number of estimators was set to 5, the leaf size to 30 and the type of distance calculation was <i>'Euclidean distance'</i> (p=2), all default parameters of the classifier.</div><br>\n",
    "    \n",
    "<b> Random Forest (RF)</b>\n",
    "<div style=\"text-align: justify\"> In the classification report of the training and test sets, for most of the classes, accuracy, recall and F1 score are slightly lower in the test set compared to the training set. This was expected, as models tend to generalise slightly worse with unseen data. However, the drop in performance is not significant, which indicates that the model still performs well on the test set. Furthermore, the accuracy obtained for the training set was 99%, while 98% for the test set.<br><br>\n",
    "    \n",
    "<div style=\"text-align: justify\">As can be seen in the validation set report, the model was not able to correctly classify instances of the <i>'Impact'</i> class, performing very poorly (0%) on all precision, recall and F1 score metrics. This result could be due to the default parameters set to train the model.  Tree depth is one of the most important parameter for tuning the model, as it sets the stop condition that limits the number of splits or levels deep a decision tree can go.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">To enhance the classifier results, it is necessary to adjust the maximum depth of the decision trees when performing hyperparameter fitting for a random forest model. The <i>'weighted avg'</i> metric also showed a decrease in performance on the test set, indicating that the model does not perform as well on the test set across all classes, considering the distribution of classes. Overall, the performance metrics on the test set remain high, indicating that the overfitting is not critical. </div><br>\n",
    "\n",
    "<b> K-Nearest Neighbor (KNN)</b>\n",
    "\n",
    "<div style=\"text-align: justify\">On the other hand, the classification report obtained for the KNN classifier showed a small difference between the training set and the validation set, for the test set the values obtained for each of the metrics; accuracy, recall, f1-score, were slightly lower compared to the results obtained for the training set.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> For the <i>'Impact'</i> class, the metrics derived in the training set were significantly lower compared to the validation set, for all metrics. This improvement in the accuracy, recall and f1-score parameters for the <i>'Impact'</i> class in the test set indicates that the model's predictions for this class are more accurate and reliable when evaluated with new, unseen data.  Producing a recall of 50%, which means, that the model only correctly predicted this class for 50% of the evaluated intents. </div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">The difference obtained in this class for the training and validation sets suggests that the model did not sufficiently fit the <i>'Impact'</i> class during training, and then after the selection of the nearest neighbour from the test set, the model adjusted its predictions to better capture the features of the <i>'Impact'</i> class. However, it is important to note that the KNN classifier does not fit the data, it does not learn from the model, it only calculates the distance to the nearest points and selects the class according to the majority result of the nearest neighbours.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> In general, the observed results do not indicate underfitting or overfitting, in fact, the average accuracy obtained in the classification report was 99%, matching with the obtained in the calculated accuracy score (98% for both sets). The high values of the <i>'micro-average'</i> in both sets suggest a good overall performance of the model. While the <i>'macro-average'</i> values are higher only in the test set, indicating that the model performs better after calculating the Euclidean distance of each point.</div><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Tune the hyper-parameters of the models through cross-validation. How do performance vary?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment, a value of 5 K-fold cross-validation was selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score\n",
    "\n",
    "scorer = make_scorer(f1_score, average='macro')\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(X_train)\n",
    "\n",
    "for i, (train_index,test_index) in enumerate(kf.split(X_train)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(model_rf, X_train, y_train_mlb, cv=kf, scoring = scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(knn, X_train, y_train_mlb, cv=kf, scoring=scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">Hyperparameters are settings that control the learning process of machine learning models. While the parameters are learned during the training process, the hyperparameters are set before the training starts. Therefore, in order to find the parameters that best fit the performance of the model, the GridSearch technique was applied. This technique applies all possible combinations of hyperparameters, resulting in a set of parameters that will improve the performance of the model.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRID SEARCH\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "params = {'n_estimators': [10, 50, 80], 'max_depth': [15, 25, 50] } #'criterion' :['gini', 'entropy']}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "gs_rf = GridSearchCV(model_rf, param_grid = params, scoring='f1_macro', cv = 5, verbose = 1) \n",
    "# scoring='accuracy'\n",
    "# cv: that's the number of fold for the cross-validation\n",
    "# verbose: specifies the verbosity level of the GridSearchCV object. \n",
    "\n",
    "# Trainning the model\n",
    "st = time.time()\n",
    "gs_rf.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print(r'Time used by Grid Search:', elapsed_time,'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_rf = gs_rf.best_params_\n",
    "print(r'The best combination of parameters the Grid Search has found is:', best_params_rf)\n",
    "print(\"Best F1-Score: {:.2f}\".format(gs_rf.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask to the prof which graph we should use -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the f1 macro reached for each combination\n",
    "y = gs_rf.cv_results_[\"mean_test_score\"].tolist()\n",
    "x = [i for i in range (1, len(y)+1)]\n",
    "mean_test_score_df = pd.DataFrame()\n",
    "mean_test_score_df[\"f1_macro\"] = y\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x = mean_test_score_df.index, y = \"f1_macro\", data = mean_test_score_df, color='blue')\n",
    "\n",
    "# Add a title and labels to the plot\n",
    "plt.title('F1-macro Scores for Different Parameters')\n",
    "plt.xlabel('Combination')\n",
    "plt.ylabel('F1-macro Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rf = pd.DataFrame(gs_rf.cv_results_)\n",
    "results_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a pivot table \n",
    "scores_rf = results_rf.pivot(index='param_max_depth', columns='param_n_estimators', values='mean_test_score')\n",
    "scores_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scores_rf, annot=True, cmap='viridis', fmt='.5g')\n",
    "plt.xlabel('param_max_depth')\n",
    "plt.ylabel('param_n_estimators')\n",
    "plt.title('Mean F1-Score over all folds for each combination of parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRID SEARCH tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'n_neighbors': [5, 7, 9], 'leaf_size': [5, 15, 20]} #'metric': ['euclidean', 'manhattan']} \n",
    "grid_search_knn = GridSearchCV(knn, params, scoring='f1_macro', cv = 5, verbose=1)\n",
    "# scoring = 'accuracy'\n",
    "\n",
    "st = time.time()\n",
    "grid_search_knn.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print(r'Time used by Grid Search:', elapsed_time,'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_knn = grid_search_knn.best_params_\n",
    "print(r'The best combination of parameters the Grid Search has found is:', best_params_knn)\n",
    "print(\"Best F1-Score: {:.2f}\".format(grid_search_knn.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aks to the prof which graph we should use --------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the f1 macro reached for each combination\n",
    "y = grid_search_knn.cv_results_[\"mean_test_score\"].tolist()\n",
    "x = [i for i in range (1, len(y)+1)]\n",
    "mean_test_score_df = pd.DataFrame()\n",
    "mean_test_score_df[\"f1_macro\"] = y\n",
    "#print(mean_test_score_df)\n",
    "\n",
    "sns.barplot(x = mean_test_score_df.index, y = \"f1_macro\", data = mean_test_score_df, color='blue')\n",
    "\n",
    "# Add a title and labels to the plot\n",
    "plt.title('F1-macro Scores for Different Parameters')\n",
    "plt.xlabel('Combination')\n",
    "plt.ylabel('F1-macro Score')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_knn = pd.DataFrame(grid_search_knn.cv_results_)\n",
    "results_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a pivot table before create the heatmap\n",
    "scores_knn = results_knn.pivot(index='param_leaf_size', columns='param_n_neighbors', values='mean_test_score')\n",
    "scores_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scores_knn, annot=True, cmap='viridis', fmt='.5g')\n",
    "plt.xlabel('param_n_neighbors')\n",
    "plt.ylabel('param_leaf_size')\n",
    "plt.title('Mean F1-score over all folds for each combination of parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4. Comments on the results for each on the intents.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">The aim of this analysis is to assess the predictive capability of two models in classifying attack labels. The models will be evaluated based on the hyperparameters identified previously.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation of Random Forest with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with the hyperparameters\n",
    "model_rf_tunned = RandomForestClassifier(n_estimators = 25, max_depth = 10) # (n_estimators=30, max_depth=50)\n",
    "\n",
    "st = time.time()\n",
    "# Trainning the model\n",
    "model_rf_tunned.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print(f'Time to train the model:', elapsed_time,'seconds','\\n')\n",
    "\n",
    "# Predictions on test set\n",
    "y_test_pred_tune = model_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "accuracy = metrics.accuracy_score(y_test_mlb, y_test_pred_tune)\n",
    "print(f\"Accuracy of the 'Random Forest' model for test set: {accuracy:.2f}\",'\\n')\n",
    "\n",
    "# Evaluate performance on test set\n",
    "report_test_tune = classification_report(y_test_mlb, y_test_pred_tune, target_names=mlb.classes_, output_dict=True)\n",
    "df_report_test_tune = pd.DataFrame(report_test_tune).transpose()\n",
    "print(f'         Classification Report Trainning Set', '\\n')\n",
    "print(df_report_test_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the heatmap of the correlation matrix\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(df_report_test_tune.loc[\"Defense Evasion\" : \"Persistence\"], cmap='Blues', annot=True, vmin=.0, vmax=1,fmt='.3f')\n",
    "plt.xlabel('Intents')\n",
    "plt.ylabel('Evaluation technique')\n",
    "plt.title('Intents classification report')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evalutaing of K-Nearest Neighbors with tuned parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the k-NN model\n",
    "knn_tune = KNeighborsClassifier(leaf_size=5, n_neighbors=5)\n",
    "\n",
    "# Train the model on the training data\n",
    "t = time.time()\n",
    "knn_tune.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print(r'Time to train the model:', elapsed_time,'seconds','\\n')\n",
    "\n",
    "# Generate predictions on the test set\n",
    "predictions_knn_tune = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "accuracy_knn_tune = knn.score(X_test, y_test_mlb)\n",
    "print(f\"Accuracy of the k-NN model: {accuracy_knn_tune:.2f}\",'\\n')\n",
    "\n",
    "report_knn_tune = classification_report(y_test_mlb, predictions_knn_tune, target_names = mlb.classes_, output_dict=True)\n",
    "df_report_knn_tune = pd.DataFrame(report_knn_tune).transpose()\n",
    "print(\"              Classification Report for KNN\",'\\n')\n",
    "print(df_report_knn_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the heatmap of the correlation matrix\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(df_report_knn_tune.loc[\"Defense Evasion\" : \"Persistence\"], cmap='Blues', annot=True, vmin=.0, vmax=1,fmt='.3f')\n",
    "plt.xlabel('Intents')\n",
    "plt.ylabel('Evaluation technique')\n",
    "plt.title('Classification report')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to write how the hyperparameter tunning improves the result; especially in the intent Impact, for both models... I'll do it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5 Explore the possible features: try combining features differently, e.g., does tf-idf improve or worsen performance? Think about the problem and summarize the ways you have tried (even those that did not work).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"><b>First Attempt</b></div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">For the first attempt, 33 features were used for the training set. The classifiers selected to perform the predictions were 'Random Forest' and 'K-Nearest Neighbor' with both models using the default parameters. In the performance evaluation, an accuracy of 98% was obtained for both models in the validation set.  While in the training set it reached 99% for RF and 98% for KNN. The decrease in accuracy achieved by RF in the validation set suggests a slight overfitting, however, it is a tolerable value that assumes that the model still performs well on the test set.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">According to the classification report, for almost all attempts the values of precision, recall and f1-scores reached high percentages, around 98.9%.  Except for the 'Impact' and 'Harmless' intents when applying RF as a model, the results obtained for these classes were quite lower compared to those obtained for the other classes, both in the training set and in the validation set, where 0% was obtained in each metric. The model presented a very poor performance when trying to classify these two classes.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">As for tuning techniques, K-fold cross-correlation and Grid Search were applied to see if the results obtained previously could be improved. However, when looking at the results obtained with 5 folds, the performance decreased considerably reaching an average of 77% accuracy. Suggests that the model may be in overfitting. </div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">Therefore, for the next attempt we consider a reduction of the dimensionality of the data set to improve generalization.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"><b>Second attempt</b></div><br>\n",
    "<div style=\"text-align: justify\">For the second attempt, the number of features was reduced to only 12 for the training set.  The classifiers used in the previous attempt, RF and KNN, were kept for this evaluation with both using their default parameters. The results obtained in the performance evaluation, the accuracy did not change from the previous attempt, reaching 98% for the RF case and 98% for the KNN for both the training and validation sets, indicating that the models continue to make good predictions for the classes.<br>\n",
    "<br>\n",
    "<div style=\"text-align: justify\">If we take a look at the classification report for almost all attempts, the values for accuracy, recall and f1-scores reached high percentages, around 98%. Nevertheless, the performance of the 'Impact' class was very poor in both models for the classification obtained in the validation data, with 0% for every metric (precision, recall and f1-score), indicating that the number of features selected to train both models was not sufficient to be able to correctly classify this class.<br><br>\n",
    "<div style=\"text-align: justify\">In both training and validation reports, the F1 score obtained for the classes \"Impact\" and \"Harmless\" was very low compared to the other classes. It seems that for both models, these two classes are the most difficult to classify correctly. This could be due to the fact that, the number of data selected during the splitting of the training set did not cover enough samples of these two classes, as they are the least sampled attempts of the whole dataset. </div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">After conducting multiple tests and adjusting the number of features in the training set, the results showed similar levels of accuracy. However, upon further analysis of the classification report parameters, it was found that the highest metrics for accuracy, recall, and F1-score were achieved with a number of features greater than 22. The F1-score parameter indicated that the models performed better in classifying samples belonging to the 'Impact' and 'Harmless' classes, which were more difficult to detect in almost all the tests. </div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <left><b><font size=4>Section 3 – Unsupervised Learning – Clustering<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">Cluster the attacks according to their characteristics. Choose at least 2 Clustering Algorithms, and for each of them solve the following points.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = result_df_corr.explode(\"Set_Fingerprint\").reset_index()\n",
    "df_exploded = df_exploded.drop(columns=['index'])\n",
    "#features_exploted = result_df_exploded.drop(columns=[\"session_id\", \"full_session\", \"first_timestamp\", \"Set_Fingerprint\", \"bag_of_words\", \"tfid\"])\n",
    "\n",
    "# We should use the stardardized data (X and y features)\n",
    "labels = df_exploded[\"Set_Fingerprint\"]\n",
    "result_df_exploded = df_exploded.filter(features_names)\n",
    "result_df_exploded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> Two clustering algorithms will be implemented to our dataset, the chosen were MiniBatchK-Means and Gaussian Mixture Model (GMM). <br>\n",
    "First of all we have to select the features to be evaluate, <b>'Pricipal Component Analysis (PCA)'</b> was used to reduce the dimensionality of the data.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction by applying 'PCA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensionality using PCA----------------------------------------\n",
    "pca = PCA(n_components=10)        # Adjust the number of components as needed\n",
    "reduced_data = pca.fit_transform(result_df_exploded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Means with n_clusters = 3\n",
    "# cl_labels3 = kmeans.fit_predict(features_exploted) # Get clusters ID\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "cl_labels3 = kmeans.fit(reduced_data) # Using the features selected\n",
    "\n",
    "# print the clustered labels\n",
    "print('The clustered labels are:\\n', kmeans.labels_)\n",
    "print()\n",
    "\n",
    "# print the centroid of each feature for each cluster\n",
    "print('The centroids are:\\n', kmeans.cluster_centers_)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised metric  (it takes approx 2500 sec o 3000 sec)\n",
    "silhouette  = silhouette_score(reduced_data, kmeans.labels_)\n",
    "\n",
    "# Supervised metrics\n",
    "ri = rand_score(np.ravel(labels), kmeans.labels_)\n",
    "ari = adjusted_rand_score(np.ravel(labels), kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('k-Means with 3 clusters')\n",
    "(unique, counts)=np.unique(kmeans.labels_, return_counts=True)\n",
    "print(\"Size of each cluster: \", counts)\n",
    "print(f'k_means clustering error: {round(kmeans.inertia_, 2)}')\n",
    "print(f'Silhouette: {round(silhouette, 2)}')\n",
    "print(f'RI: {round(ri, 2)}')\n",
    "print(f'ARI: {round(ari, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian mixture model (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=3)\n",
    "gmm.fit(result_df_exploded)           # Get clusters ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the clustered labels\n",
    "gmm_labels = gmm.predict(result_df_exploded)\n",
    "print('The clustered labels are:\\n', gmm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised metric\n",
    "silhouette  = silhouette_score(result_df_exploded, gmm_labels)\n",
    "log_l = gmm.score(result_df_exploded)\n",
    "\n",
    "# Supervised metrics\n",
    "ri = rand_score(np.ravel(labels), gmm_labels)\n",
    "ari = adjusted_rand_score(np.ravel(labels), gmm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report effective size\n",
    "print(\"Effetive size of each cluster: \", gmm.weights_)\n",
    "# report usupervised and supervised metric\n",
    "print(f'GMM total log-likelihood score: {round(log_l, 2)}')\n",
    "print(f'Silhouette: {round(silhouette, 2)}')\n",
    "print(f'RI: {round(ri, 2)}')\n",
    "print(f'ARI: {round(ari, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1. Determine the number of clusters: This can be done using methods like the elbow method or\n",
    "silhouette analysis. Explain your reasoning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> The clustering error is strongly dependent form the number of cluster. \n",
    "The best situation, with the lowest error possible, is obiuously when we have the same number of clusters of the datapoins. Actually in this situation we're not even performing clustering, but the clustering error will be zero.<br>\n",
    "<br>\n",
    "Now we want to find the number of clusters that gives us the best results, so with the lowest clustering error. \n",
    "We'll follow 2 approches for each clustering algorithm applied, elbow method and validation error. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Elbow Method  (260 sec)\n",
    "n_cluster_list=[]\n",
    "inertia_list=[]\n",
    "shs_list = []\n",
    "ri_list = []\n",
    "ari_list = []\n",
    "\n",
    "for n_clusters in range(3, 16):\n",
    "    kmeans_new = KMeans(n_clusters=n_clusters)\n",
    "    cl_labels = kmeans_new.fit_predict(reduced_data)\n",
    "    \n",
    "    # Unsupervised metric  (If we evaluate the silhoutte it takes too long)\n",
    "    # silhouette = silhouette_score(reduced_data, cl_labels)\n",
    "    # n_cluster_list.append(n_clusters)\n",
    "    # shs_list.append(silhouette)\n",
    "    \n",
    "   # Rand Index and Adjusted Rand Index:\n",
    "    ri_list.append(rand_score(np.ravel(labels), cl_labels))\n",
    "    ari_list.append(adjusted_rand_score(np.ravel(labels), cl_labels))\n",
    "    \n",
    "    # For Elbow Method (Inertia):\n",
    "    inertia_list.append(kmeans_new.inertia_)\n",
    "    n_cluster_list.append(n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elbow Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "# Plot k-Means clustering error \n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list, inertia_list, marker='o', markersize=5, color='blue')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('k-Means clustering error')\n",
    "plt.title('k-Means Clustering Error Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RI\n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list, ri_list, marker='o', markersize=5, color='blue')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('RI')\n",
    "plt.title('Rand Index Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ARI\n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list, ari_list, marker='o', markersize=5, color='blue')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('ARI')\n",
    "plt.title('Adjusted Rand Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THE CODE BELOW: IT's used if we would have chosen to perform the silhouette analysis, but it is not mandatory, of course we have to justify why we do not choose this one, computational time and so on...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette\n",
    "# Get n_clusters leading to the highest silhouette\n",
    "best_sh = np.max(shs_list)\n",
    "best_n = n_cluster_list[np.argmax(shs_list)]\n",
    "print(\"best k: \",best_n, \" with corresponding silhouette: \", best_sh)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list,shs_list, marker='o', markersize=5)\n",
    "plt.scatter(best_n, best_sh, color='r', marker='x', s=90)\n",
    "plt.grid()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Mixture Model (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cluster_list=[]\n",
    "shs_list = []\n",
    "ri_list = []\n",
    "ari_list = []\n",
    "log_l_list=[]\n",
    "\n",
    "for n_clusters in range(3, 16):\n",
    "    gmm = GaussianMixture(n_components=n_clusters)\n",
    "    cl_labels = gmm.fit_predict(X_s)\n",
    "    \n",
    "    # silhouette  = silhouette_score(X_s, cl_labels)\n",
    "    # n_cluster_list.append(n_clusters)\n",
    "    # shs_list.append(silhouette)\n",
    "    \n",
    "    ri_list.append(rand_score(np.ravel(y), cl_labels))\n",
    "    ari_list.append(adjusted_rand_score(np.ravel(y), cl_labels))\n",
    "    log_l_list.append(gmm.score(X_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elbow Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Has the performance improved also on the other metrics? Plot the other metrics for the different values of n_cluster.\n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Plot GMM total log-likelihood score\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list,log_l_list, marker='o', markersize=5)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('GMM total log-likelihood score')\n",
    "plt.show()\n",
    "\n",
    "# Plot ARI\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list,ari_list, marker='o', markersize=5)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('ARI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Plot the silhouette score for the different values of n_cluster. \n",
    "# What is the best value of k, leading to the highest Silhouette.\n",
    "\n",
    "# Get n_clusters leading to the highest silhouette\n",
    "best_sh= np.max(shs_list)\n",
    "best_n=n_cluster_list[np.argmax(shs_list)]\n",
    "print(\"best k: \",best_n, \" with corresponding silhouette: \", best_sh)\n",
    "\n",
    "# Plot\n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list,shs_list, marker='o', markersize=5)\n",
    "plt.scatter(best_n, best_sh, color='r', marker='x', s=90)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2. Tune other hyper-parameters, if any.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tunning the hyperparameter of K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Elbow Method\n",
    "n_cluster_list=[]\n",
    "inertia_list=[]\n",
    "shs_list = []\n",
    "ri_list = []\n",
    "ari_list = []\n",
    "\n",
    "for n_clusters in range(3, 16):\n",
    "    kmeans_tunning = KMeans(n_clusters=n_clusters, init='k-means++', random_state=None, n_init=1)\n",
    "    labels_tunning = kmeans_tunning.fit_predict(reduced_data) # Using the features selected\n",
    "    # kmeans_new = KMeans(n_clusters=n_clusters)\n",
    "    # cl_labels = kmeans_new.fit_predict(reduced_data)\n",
    "    \n",
    "    # Rand Index and Adjusted Rand Index:\n",
    "    ri_list.append(rand_score(np.ravel(labels), labels_tunning))\n",
    "    ari_list.append(adjusted_rand_score(np.ravel(labels), labels_tunning))\n",
    "    \n",
    "    # For Elbow Method (Inertia):\n",
    "    inertia_list.append(kmeans_tunning.inertia_)\n",
    "    n_cluster_list.append(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "# Plot k-Means clustering error \n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list, inertia_list, marker='o', markersize=5, color='orange')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('k-Means clustering error')\n",
    "#plt.title('k-Means Clustering Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RI\n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list, ri_list[:13], marker='o', markersize=5, color='orange')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('RI')\n",
    "plt.title('Rand Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ARI\n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list, ari_list, marker='o', markersize=5, color='orange')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('ARI')\n",
    "plt.title('Adjusted Rand Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3. Visualize the clusters through t-SNE visualization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the k-mean with the best hyper-parameters found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=15, init='k-means++', random_state=None, n_init=1)\n",
    "cl_labels3 = kmeans.fit_predict(reduced_data) # Using the features selected\n",
    "\n",
    "# print the clustered labels\n",
    "print('The clustered labels are:\\n', kmeans.labels_)\n",
    "print()\n",
    "\n",
    "# print the centroid of each feature for each cluster\n",
    "print('The centroids are:\\n', kmeans.cluster_centers_)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of t-SNE is to take a set of points in a high-dimensional space and find a faithful representation of those points in a lower-dimensional space, typically the 2D plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.DataFrame(reduced_data)\n",
    "labels_df['cluster'] = cl_labels3\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = labels_df.iloc[:,:-1]\n",
    "print(X)\n",
    "tsne = TSNE(n_components=2).fit_transform(X)\n",
    "\n",
    "df_tsne = pd.DataFrame(tsne)\n",
    "df_tsne[\"cluster\"] = cl_labels3\n",
    "df_tsne.columns = [\"x1\", \"x2\", \"cluster\"]\n",
    "df_tsne\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "sns.scatterplot(data=df_tsne, x='x1', y='x2', hue='cluster', legend=\"full\", alpha=0.5)\n",
    "ax.set_title('2D TSNE cluster visualization')\n",
    "\n",
    "\n",
    "ax.set_xlim([-200, 300])  \n",
    "ax.set_ylim([-220, 260])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4. Cluster analysis. Analyze the characteristics of each cluster. This might involve examining the most frequent words in each cluster (try word cloud). Try to understand which are the most\n",
    "representative.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5. Do clusters reflect intent division, i.e., are the clusters homogeneous in terms of intents? How are intents divided into the clusters?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.6. Find clusters of similar attacks, study their sessions and try to associate with them specific categories of attacks (more fine grained than the ones of MITRE ATT&CK Tactics). As an example, see the image below, where we perform a similar exercise (through graph community detections). NOTE: you do not have to do this exercise for all the clusters, but only on some examples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><left><b><font size=4>Section 4 – Language Models exploration<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">Experiment language models for solving the same supervised task as in Section 2. In this task, the objective is to harness the capabilities of language models like Bert or Word2Vec, for supervised learning (assign intents to sessions). \n",
    "<br><br> Two interesting concepts play a role when we use neural networks:\n",
    "<div style=\"text-align: justify\"><br><b>1)</b> It is possible to do transfer learning, i.e., to take a model that have been trained with other enormous datasets by Big Tech companies, and we can do fine-tuning i.e., to train this model starting from its pre-trained version.\n",
    "<br><b>2)</b> In NLP tasks, words/documents are transformed into vectors (encoding) and this task is Unsupervised, so we can use a much larger amount of data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"><b>4.1. If you choose Doc2Vec: pretrain Doc2Vec on body column of the session text. If you chose Bert: take the pretrained Bert model like in this example. (NB: In this tutorial they used BertForSequenceClassification, but if you want to continue with step 2, you must take an other Bert implementation from HuggingFace)</div></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_parquet('ssh_attacks.parquet')\n",
    "data = df_original.copy()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Filter out non-alphabetic characters and convert to lowercase\n",
    "    cleaned_text = ' '.join(word.lower() for word in text.split() if word.isalpha())\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply text cleaning to the 'full_session' column\n",
    "data['cleaned_session'] = data['full_session'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize 'cleaned_session' texts and create tagged documents\n",
    "tagged_data = [TaggedDocument(words=session.split(), tags=[str(i)]) for i, session in enumerate(data['cleaned_session'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building and Training Doc2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">Cleaning the text data by removing non-alphabetic characters and converting text to lowercase. The Gensim library was used to train a Doc2Vec model on the cleaned text data. A vocabulary was buildt and the Doc2Vec model was trained to generate the vector of embeddings for each session text.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Doc2Vec model\n",
    "doc2vec_model = Doc2Vec(vector_size=100, window=5, min_count=1, epochs=20, dm=1)\n",
    "\n",
    "# Build vocabulary\n",
    "doc2vec_model.build_vocab(tagged_data)\n",
    "\n",
    "# Train the model\n",
    "doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "doc2vec_model.save('trained_doc2vec_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2. Add a last Dense Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">We have trained the Doc2Vec model, which generated embeddings for our text data. Now, to perform classification, we will build a simple Neural Network that will take these embeddings and will add a dense layer for the classification task. The dense layer will have as many neurons as the number of classes we want to predict.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a neural network model using TensorFlow/Keras. Set up the architecture by adding a Dense layer with the input dimension being the size of the Doc2Vec vectors and a final Dense layer with softmax activation for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the Doc2Vec embeddings as the input layer\n",
    "input_dim = doc2vec_model.vector_size\n",
    "model.add(Dense(input_dim, input_shape=(input_dim,), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the final dense layer for classification\n",
    "num_classes = 7\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Softmax activation for multi-class classification\n",
    "#This code adds the final dense layer for classification on top of the Doc2Vec embeddings. num_classes represents the number of output classes we have in our classification task (in our case, the 7 different intents). The softmax activation function is used here as it's suitable for multi-class classification tasks, providing probabilities for each class.\n",
    "#The purpose of this code is to create a neural network architecture suitable for classification using the Doc2Vec embeddings as input features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">To compile the model, we'll need to set a few parameters:<br>\n",
    "\n",
    "- Optimizer: The optimizer adjusts the weights during training to minimize the loss function.\n",
    "- Loss Function: For multi-class classification, 'categorical_crossentropy' is commonly used.\n",
    "- Metrics: These are used to judge the performance of the model. For classification tasks, 'accuracy' is a standard metric.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Categorical Crossentropy: This loss function is suitable for multi-class classification problems. It measures the dissimilarity between the true distribution and the predicted distribution of the classes.\n",
    "#Accuracy: It calculates the accuracy of the model, i.e., the number of correctly predicted instances divided by the total number of instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3. Fine-tune the last layer of the network on the supervised training set for N epochs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">Once the dataset was divided into training and validation sets. Converted the data into TensorFlow tensors. Fit the neural network model using the training set, specifying the number of epochs and batch size.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained Doc2Vec model\n",
    "doc2vec_model = Doc2Vec.load('trained_doc2vec_model.model') \n",
    "\n",
    "# Function to infer vectors for each document\n",
    "def infer_vector(text):\n",
    "    return doc2vec_model.infer_vector(text.split())\n",
    "\n",
    "# Apply inference to each text and store the vectors in a new column\n",
    "data['doc2vec_vectors'] = data['cleaned_session'].apply(infer_vector)\n",
    "\n",
    "# Extract vectors and convert to array for modeling\n",
    "X = np.array(data['doc2vec_vectors'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['doc2vec_vectors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['doc2vec_vectors']  \n",
    "y = data['Set_Fingerprint'] \n",
    "\n",
    "# Split the data into training and test sets (70% train, 30% test)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Print the shapes \n",
    "print(f\"X_train shape: {X_train1.shape}, y_train shape: {y_train1.shape}\")\n",
    "print(f\"X_test shape: {X_test1.shape}, y_test shape: {y_test1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y_train1_mlb = mlb.fit_transform(y_train1)\n",
    "y_test1_mlb = mlb.transform(y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X_train_array = np.array(X_train1.tolist())\n",
    "X_test_array = np.array(X_test1.tolist())\n",
    "\n",
    "X_train_tensor = tf.convert_to_tensor(X_train_array)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test_array)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train1_mlb)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test1_mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_train1_mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import clone_model\n",
    "cloned_model = clone_model(model)\n",
    "cloned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = cloned_model.fit(X_train_tensor, y_train_tensor, epochs=10, batch_size=32, validation_data=(X_test_tensor, y_test_tensor))\n",
    "#Epoch: An epoch represents one complete pass through the entire training dataset.\n",
    "#Number of Epochs: It determines the number of times the learning algorithm will work through the entire training dataset\n",
    "#Batch: The training data is divided into batches. The model is trained on each batch, and the weights are updated after each batch.\n",
    "#Batch Size: It's the number of samples processed before the model is updated. Smaller batch sizes offer faster training but may be less stable than larger batch sizes.\n",
    "#model.fit(): This function fits the model on the training data. It iterates over a fixed number of epochs and updates the model's weights based on the backpropagation of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">When using <i>'tf.function'</i>, TensorFlow creates a graph representation of the computation for optimization. However, certain operations, especially those involving variable creation, can cause issues within the graph when the function is called multiple times. <br><br>\n",
    "Creating a new model instance or cloning the model before train it again helps to avoid conflicts caused by the pre-existing variables or the TensorFlow graph. This approach prevents potential issues that might arise from reusing the same model object within a <i>'tf.function'</i> context. Essentially, it ensures that the new training session starts with a fresh model instance, avoiding any residual state from previous training sessions that might interfere with the current one.</div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4 Plot the learning curves on training and validation set. After how many epochs should we stop the training?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">The Matplotlib library was used to create visualisations of training and validation losses, as well as training and validation accuracy over epochs. These plots are helpful to understand how the model learns over time, showing convergence and possible overfitting or underfitting.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">(Learning curves are used to understand the changes in 'loss' and 'accuracy' at each epoch. If the training accuracy is high but the validation accuracy is low, it may indicate overfitting, whereas consistently low accuracy for both may suggest underfitting).</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">[In summary, the process involved preparing the text data, training a Doc2Vec model to generate embeddings, constructing and training a neural network model for intent classification, and visualizing the model's learning progress. The aim is to build a model that effectively identifies attack session intents using text data and Doc2Vec embeddings.]</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the training history\n",
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "training_accuracy = history.history['accuracy']\n",
    "validation_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_loss, label='Training Loss')\n",
    "plt.plot(validation_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_accuracy, label='Training Accuracy')\n",
    "plt.plot(validation_accuracy, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><b><font size=4>Final Considerations<b><left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
