{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, make_scorer, f1_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"ssh_attacks_decoded_splitted.parquet\"):\n",
    "    raise Exception(\"You should run the preprocessing file\")\n",
    "df = pd.read_parquet(\"ssh_attacks_decoded_splitted.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (os.path.isfile(\"df_features_bow.parquet\") and os.path.isfile(\"df_features_tfidf.parquet\")):\n",
    "    raise Exception(\"You should run the section 1 before\")\n",
    "df_features_bow = pd.read_parquet(\"df_features_bow.parquet\")\n",
    "df_features_tfidf = pd.read_parquet(\"df_features_tfidf.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=4>Section 2 – Supervised Learning – Classification<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"text-align: justify\"> Classify the tactics of an attack session, based on the used words in the text and also possibly on time. Notice that each session have multiple labels. Hence you can decompose the problem into multiple binary classification problems. For each attack session, you have to solve the 7 binary classification problem, one for each possible label {'Persistence', 'Discovery', 'Defense Evasion', 'Execution', 'Impact', 'Other', 'Harmless'}. </div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Perform a split to segment the dataset into training and test dataset. If you want to standardize your dataset, fit the scaler on training set and transforming both training and test. Notice that the sklearn implementation of tf-idf already performs the standardization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = df_features_tfidf.explode(\"Set_Fingerprint\").reset_index()\n",
    "df_exploded = df_exploded.drop(columns=['index'])\n",
    "\n",
    "\n",
    "df_persistence = df_exploded[df_exploded[\"Set_Fingerprint\"] == \"Persistence\"].sample(frac=0.1).copy()\n",
    "df_discovery = df_exploded[df_exploded[\"Set_Fingerprint\"] == \"Discovery\"].sample(frac=0.1).copy()\n",
    "df_defenseEvasion = df_exploded[df_exploded[\"Set_Fingerprint\"] == \"Defense Evasion\"].sample(frac=0.1).copy()\n",
    "df_execution = df_exploded[df_exploded[\"Set_Fingerprint\"] == \"Execution\"].sample(frac=0.1).copy()\n",
    "df_impact = df_exploded[df_exploded[\"Set_Fingerprint\"] == \"Impact\"].sample(frac=0.1).copy()\n",
    "df_other = df_exploded[df_exploded[\"Set_Fingerprint\"] == \"Other\"].sample(frac=0.1).copy()\n",
    "df_harmless= df_exploded[df_exploded[\"Set_Fingerprint\"] == \"Harmless\"].sample(frac=0.1).copy()\n",
    "\n",
    "\n",
    "sampled_df = pd.concat([df_persistence, df_discovery, df_defenseEvasion, df_execution, df_impact, df_other, df_harmless], ignore_index=True)\n",
    "\n",
    "X = sampled_df.drop(columns=\"Set_Fingerprint\")\n",
    "y = sampled_df[\"Set_Fingerprint\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count         39055\n",
       "unique            7\n",
       "top       Discovery\n",
       "freq          16249\n",
       "Name: Set_Fingerprint, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count         16739\n",
       "unique            7\n",
       "top       Discovery\n",
       "freq           6965\n",
       "Name: Set_Fingerprint, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23214"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16249 + 6965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The size of trainning set is: {len(X_train)}\")\n",
    "print(f\"The size of trainning set is: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "##### Standardization of the Numerical Features\n",
    "\n",
    "As the TF-IDF pre-processing was applied previously to all the sessions, the data considered as features was already standardized. \n",
    "\n",
    "##### Standardization of the Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y_train_mlb = mlb.fit_transform(y_train)\n",
    "y_test_mlb = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Techniques** \n",
    "<br>\n",
    "<div style=\"text-align: justify\"> A <b>MultiLabelBinarizer</b> is a transformer that is used for multi-label classification problems, in order to handle the cases where each sample belongs to multiple classes simultaneously. The purpose of MultiLabelBinarizer is to convert a collection of sequences of labels into a binary matrix format. The binary classification of each label in the 'Set_Fingerprint' column was performed by converting the multi-class label matrix into a binary matrix, where each column represents one of the possible classes and each row represents one instance. </div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> <b>TF-IDF </b> (explain technique here) </div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> MultiLabelBinarizer is used to handle categorical variables before fitting a model, as most machine learning algorithms can only handle numerical data.</div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Choose at least 2 ML methods, and perform the model training, with default parameter\n",
    "configuration, evaluating the performance on both training and test set. Output the confusion\n",
    "matrix and classification report. Do you observe overfitting or under-fitting? Which model\n",
    "generates the best performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><left><b><font size=4> Random Forest (RF)<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">Random Forest (RF) serves as a classification model that constructs a collection of decision trees (DT) using a randomly chosen subset of the given training set. The model aggregates the individual decisions made by each decision tree and combines their votes to make the ultimate prediction.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100) \n",
    "\n",
    "st = time.time()\n",
    "rf.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print(f\"Time to train the model: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predictions = rf.predict(X_train)\n",
    "y_test_predictions = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the train data\n",
    "accuracy = rf.score(X_train, y_train_mlb)\n",
    "print(f\"Accuracy of the 'Random Forest' model for the training set: {accuracy:.2f}\")\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "accuracy = rf.score(X_test, y_test_mlb)\n",
    "print(f\"Accuracy of the 'Random Forest' model for test set: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=3 >Classification Report<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME WE SHOULD SPLIT BETTER THE TRAIN AND THE TEST SET, THE WARNING COMES OUT BECAUSE y_train_mlb AND \n",
    "# y_train_predictions DON'T HAVE THE SAME LABELS (IMPACT AND OTHER ARE VERY FEW)\n",
    "# Evaluate performance on training set\n",
    "report_training = classification_report(y_train_mlb, y_train_predictions, target_names=mlb.classes_, output_dict=True, zero_division=1)\n",
    "df_report_training = pd.DataFrame(report_training).transpose()\n",
    "df_report_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Set (Test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on test set\n",
    "report_test = classification_report(y_test_mlb, y_test_predictions, target_names=mlb.classes_, output_dict=True, zero_division=1)\n",
    "df_report_test = pd.DataFrame(report_test).transpose()\n",
    "df_report_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=3> Confusion Matrix <b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_rf_train = multilabel_confusion_matrix(y_train_mlb, y_train_predictions)\n",
    "\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    print(f\"Confusion Matrix for '{label}':\")\n",
    "    print(confusion_rf_train[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.title(f\"Confusion Matrix for '{label}'\")\n",
    "    plt.imshow(confusion_rf_train[i], cmap='Blues', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(np.arange(2), ['Negative', 'Positive'])\n",
    "    plt.yticks(np.arange(2), ['Negative', 'Positive'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_rf_test = multilabel_confusion_matrix(y_test_mlb, y_test_predictions)\n",
    "\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    print(f\"Confusion Matrix for '{label}':\")\n",
    "    print(confusion_rf_test[i], \"\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.title(f\"Confusion Matrix for '{label}' in Test Set\")\n",
    "    plt.imshow(confusion_rf_test[i], cmap='Oranges', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(np.arange(2), ['Negative', 'Positive'])\n",
    "    plt.yticks(np.arange(2), ['Negative', 'Positive'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=4>K-Nearest Neighbors (KNN)<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">K-Nearest Neighbors (KNN) operates as a supervised learning classifier that relies on the concept of proximity to perform classifications or predictions for individual data points. Its fundamental principle is grounded in the notion that similar data points tend to cluster together. In the context of classification tasks, KNN assigns a class label to a data point by considering the majority vote of its nearest neighbors. Put simply, it selects the label that is most prevalent among the neighboring data points in close proximity to the one being evaluated.</div><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "st = time.time()\n",
    "knn.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "\n",
    "elapsed_time = et - st\n",
    "print(f\"Time to train the model: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = knn.predict(X_train) \n",
    "predictions_test = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = knn.score(X_train, y_train_mlb)\n",
    "print(f\"Accuracy of the k-NN model for the training set: {accuracy:.2f}\")\n",
    "\n",
    "accuracy = knn.score(X_test, y_test_mlb)\n",
    "print(f\"Accuracy of the k-NN model for the test set: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=3 >Classification Report<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report for training set\n",
    "report_train_knn = classification_report(y_train_mlb, predictions_train, target_names=mlb.classes_, output_dict=True, zero_division=1)\n",
    "df_report_train = pd.DataFrame(report_train_knn).transpose()\n",
    "print(\"Classification Report for Trainning set:\")\n",
    "df_report_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report for test set\n",
    "report_test_knn = classification_report(y_test_mlb, predictions_test, target_names=mlb.classes_, output_dict=True, zero_division=1)\n",
    "df_report_test_knn = pd.DataFrame(report_test_knn).transpose()\n",
    "print(\"Classification Report for Test set:\")\n",
    "df_report_test_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=3> Confusion Matrix <b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code presented below prints a series of confusion matrices for each class, displaying True-Positive (top-left), False-Negative (bottom-left), False-Positive (top-right), and True-Negative (bottom-right) counts.\n",
    "- True Positives (TP): Predicted correctly as positive.\n",
    "- False Positives (FP): Predicted as positive but actually negative.\n",
    "- False Negatives (FN): Predicted as negative but actually positive.\n",
    "- True Negatives (TN): Predicted correctly as negative.\n",
    "\n",
    "Each value in the confusion matrix represents the count of instances falling into these categories for a specific label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with a confusion matrix and classification report\n",
    "confusion_knn_test = multilabel_confusion_matrix(y_test_mlb, predictions_test)\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    print(f\"Confusion Matrix for {label}:\")\n",
    "    print(confusion_knn_test[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.title(f\"Confusion Matrix for '{label}'\")\n",
    "    plt.imshow(confusion_knn_test[i], cmap='Oranges', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(np.arange(2), ['Negative', 'Positive'])\n",
    "    plt.yticks(np.arange(2), ['Negative', 'Positive'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> To know whether the model is underfitting or overfitting, we must first define these two terms. <br>\n",
    "<div style=\"text-align: justify\"><br><b>Underfitting</b> occurs when a model is too simple to capture the underlying patterns in the training data, resulting in poor performance even on the training set.  One indicator used to identify this modelling error is to look at the results; if both training and validation/testing performance are poor, the model is considered to be underfitted.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"><b>Overfitting</b> occurs when a model not only learns the underlying patterns in the training data, but also captures noise and random fluctuations, causing it to perform poorly on new and unknown data. One indicator used to identify this modelling error is to look at the results, if the model performs well on training data but poorly on validation or test data, it is considered overfitted.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">Once these two terms have been defined, it is possible to move on to the results obtained for both models. It is important to mention that, for both tests performed with the different classifiers, the default parameters were used.</div><br>\n",
    "<div style=\"text-align: justify\">For the <b>Random Forest (RF)</b> case, the default parameter implied the number of estimators equal to 100, and the tree depth was set to <i>'None'</i>. On the other hand, for the <b>K-Nearest Neighbor (KNN)</b> classifier, the number of estimators was set to 5, the leaf size to 30 and the type of distance calculation was <i>'Euclidean distance'</i> (p=2), all default parameters of the classifier.</div><br>\n",
    "    \n",
    "<b> Random Forest (RF)</b>\n",
    "<div style=\"text-align: justify\"> In the classification report of the training and test sets, for most of the classes, accuracy, recall and F1 score are slightly lower in the test set compared to the training set. This was expected, as models tend to generalise slightly worse with unseen data. However, the drop in performance is not significant, which indicates that the model still performs well on the test set. Furthermore, the accuracy obtained for the training set was 99%, while 98% for the test set.<br><br>\n",
    "    \n",
    "<div style=\"text-align: justify\">As can be seen in the validation set report, the model was not able to correctly classify instances of the <i>'Impact'</i> class, performing very poorly (0%) on all precision, recall and F1 score metrics. This result could be due to the default parameters set to train the model.  Tree depth is one of the most important parameter for tuning the model, as it sets the stop condition that limits the number of splits or levels deep a decision tree can go.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">To enhance the classifier results, it is necessary to adjust the maximum depth of the decision trees when performing hyperparameter fitting for a random forest model. The <i>'weighted avg'</i> metric also showed a decrease in performance on the test set, indicating that the model does not perform as well on the test set across all classes, considering the distribution of classes. Overall, the performance metrics on the test set remain high, indicating that the overfitting is not critical. </div><br>\n",
    "\n",
    "<b> K-Nearest Neighbor (KNN)</b>\n",
    "\n",
    "<div style=\"text-align: justify\">On the other hand, the classification report obtained for the KNN classifier showed a small difference between the training set and the validation set, for the test set the values obtained for each of the metrics; accuracy, recall, f1-score, were slightly lower compared to the results obtained for the training set.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> For the <i>'Impact'</i> class, the metrics derived in the training set were significantly lower compared to the validation set, for all metrics. This improvement in the accuracy, recall and f1-score parameters for the <i>'Impact'</i> class in the test set indicates that the model's predictions for this class are more accurate and reliable when evaluated with new, unseen data.  Producing a recall of 50%, which means, that the model only correctly predicted this class for 50% of the evaluated intents. </div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">The difference obtained in this class for the training and validation sets suggests that the model did not sufficiently fit the <i>'Impact'</i> class during training, and then after the selection of the nearest neighbour from the test set, the model adjusted its predictions to better capture the features of the <i>'Impact'</i> class. However, it is important to note that the KNN classifier does not fit the data, it does not learn from the model, it only calculates the distance to the nearest points and selects the class according to the majority result of the nearest neighbours.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> In general, the observed results do not indicate underfitting or overfitting, in fact, the average accuracy obtained in the classification report was 99%, matching with the obtained in the calculated accuracy score (98% for both sets). The high values of the <i>'micro-average'</i> in both sets suggest a good overall performance of the model. While the <i>'macro-average'</i> values are higher only in the test set, indicating that the model performs better after calculating the Euclidean distance of each point.</div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Tune the hyper-parameters of the models through cross-validation. How do performance vary?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">Hyperparameters are settings that control the learning process of machine learning models. While the parameters are learned during the training process, the hyperparameters are set before the training starts. Therefore, in order to find the parameters that best fit the performance of the model, the GridSearch technique was applied. This technique applies all possible combinations of hyperparameters, resulting in a set of parameters that will improve the performance of the model.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRID SEARCH\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "params = {'n_estimators': [3, 50, 100], 'max_depth': [100, 1000, 10000] } #'criterion' :['gini', 'entropy']}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "gs_rf = GridSearchCV(rf, param_grid = params, scoring='f1_macro', cv = 5, verbose = 1) \n",
    "# scoring='accuracy'\n",
    "# cv: that's the number of fold for the cross-validation\n",
    "# verbose: specifies the verbosity level of the GridSearchCV object. \n",
    "\n",
    "# Trainning the model\n",
    "st = time.time()\n",
    "gs_rf.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print(f\"Time to train the model: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_rf = gs_rf.best_params_\n",
    "print(f\"The best combination of parameters the Grid Search has found is: {best_params_rf}\")\n",
    "print(\"Best F1-Score: {:.2f}\".format(gs_rf.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask to the prof which graph we should use -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the f1 macro reached for each combination\n",
    "y = gs_rf.cv_results_[\"mean_test_score\"].tolist()\n",
    "x = [i for i in range (1, len(y)+1)]\n",
    "mean_test_score_df = pd.DataFrame()\n",
    "mean_test_score_df[\"f1_macro\"] = y\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x = mean_test_score_df.index, y = \"f1_macro\", data = mean_test_score_df, color='blue')\n",
    "\n",
    "# Add a title and labels to the plot\n",
    "plt.title('F1-macro Scores for Different Parameters')\n",
    "plt.xlabel('Combination')\n",
    "plt.ylabel('F1-macro Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rf = pd.DataFrame(gs_rf.cv_results_)\n",
    "results_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a pivot table \n",
    "scores_rf = results_rf.pivot(index='param_max_depth', columns='param_n_estimators', values='mean_test_score')\n",
    "scores_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scores_rf, annot=True, cmap='viridis', fmt='.5g')\n",
    "plt.xlabel('param_max_depth')\n",
    "plt.ylabel('param_n_estimators')\n",
    "plt.title('Mean F1-Score over all folds for each combination of parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_neighbors': [5, 10, 20], 'leaf_size': [10, 70, 100]}        #'metric': ['euclidean', 'manhattan']} \n",
    "grid_search_knn = GridSearchCV(knn, params, scoring='f1_macro', cv = 5, verbose=1)\n",
    "# scoring = 'accuracy'\n",
    "\n",
    "st = time.time()\n",
    "grid_search_knn.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print(f\"Time to train the model: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_knn = grid_search_knn.best_params_\n",
    "print(f\"The best combination of parameters the Grid Search has found is: {best_params_knn}\")\n",
    "print(\"Best F1-Score: {:.2f}\".format(grid_search_knn.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aks to the prof which graph we should use --------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the f1 macro reached for each combination\n",
    "y = grid_search_knn.cv_results_[\"mean_test_score\"].tolist()\n",
    "x = [i for i in range (1, len(y)+1)]\n",
    "mean_test_score_df = pd.DataFrame()\n",
    "mean_test_score_df[\"f1_macro\"] = y\n",
    "#print(mean_test_score_df)\n",
    "\n",
    "sns.barplot(x = mean_test_score_df.index, y = \"f1_macro\", data = mean_test_score_df, color='blue')\n",
    "# Add a title and labels to the plot\n",
    "plt.title('F1-macro Scores for Different Parameters')\n",
    "plt.xlabel('Combination')\n",
    "plt.ylabel('F1-macro Score')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_knn = pd.DataFrame(grid_search_knn.cv_results_)\n",
    "results_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a pivot table before create the heatmap\n",
    "scores_knn = results_knn.pivot(index='param_leaf_size', columns='param_n_neighbors', values='mean_test_score')\n",
    "scores_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scores_knn, annot=True, cmap='viridis', fmt='.5g')\n",
    "plt.xlabel('param_n_neighbors')\n",
    "plt.ylabel('param_leaf_size')\n",
    "plt.title('Mean F1-score over all folds for each combination of parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4. Comments on the results for each on the intents.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">The aim of this analysis is to assess the predictive capability of two models in classifying attack labels. The models will be evaluated based on the hyperparameters identified previously.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation of Random Forest with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with the hyperparameters\n",
    "model_rf_tunned = RandomForestClassifier(n_estimators = 50, max_depth = 10000)\n",
    "\n",
    "st = time.time()\n",
    "# Trainning the model\n",
    "model_rf_tunned.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print(f'Time to train the model:', elapsed_time,'seconds','\\n')\n",
    "\n",
    "# Predictions on test set\n",
    "y_test_pred_tune = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "accuracy = model_rf_tunned.score(X_test, y_test_mlb)\n",
    "print(f\"Accuracy of the 'Random Forest' model for test set: {accuracy:.2f}\",'\\n')\n",
    "\n",
    "# Evaluate performance on test set\n",
    "report_test_tune = classification_report(y_test_mlb, y_test_pred_tune, target_names=mlb.classes_, output_dict=True,\n",
    "                                         zero_division=1)\n",
    "df_report_test_tune = pd.DataFrame(report_test_tune).transpose()\n",
    "print(f'         Classification Report Trainning Set', '\\n')\n",
    "print(df_report_test_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the heatmap of the correlation matrix\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(df_report_test_tune.loc[\"Defense Evasion\" : \"Persistence\"], cmap='Blues', annot=True, vmin=.0, vmax=1,fmt='.3f')\n",
    "plt.xlabel('Intents')\n",
    "plt.ylabel('Evaluation technique')\n",
    "plt.title('Intents classification report')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evalutaing of K-Nearest Neighbors with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the k-NN model\n",
    "knn_tune = KNeighborsClassifier(leaf_size=10, n_neighbors=5)\n",
    "\n",
    "# Train the model on the training data\n",
    "t = time.time()\n",
    "knn_tune.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print(f\"Time to train the model: {elapsed_time} seconds\")\n",
    "\n",
    "# Generate predictions on the test set\n",
    "predictions_knn_tune = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "accuracy_knn_tune = knn.score(X_test, y_test_mlb)\n",
    "print(f\"Accuracy of the k-NN model: {accuracy_knn_tune:.2f}\",'\\n')\n",
    "\n",
    "report_knn_tune = classification_report(y_test_mlb, predictions_knn_tune, target_names = mlb.classes_, output_dict=True,\n",
    "                                       zero_division=1)\n",
    "df_report_knn_tune = pd.DataFrame(report_knn_tune).transpose()\n",
    "print(\"              Classification Report for KNN\",'\\n')\n",
    "print(df_report_knn_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the heatmap of the correlation matrix\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(df_report_knn_tune.loc[\"Defense Evasion\" : \"Persistence\"], cmap='Blues', annot=True, vmin=.0, vmax=1,fmt='.3f')\n",
    "plt.xlabel('Intents')\n",
    "plt.ylabel('Evaluation technique')\n",
    "plt.title('Classification report')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to write how the hyperparameter tunning improves the result; especially in the intent Impact, for both models... I'll do it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5 Explore the possible features: try combining features differently, e.g., does tf-idf improve or worsen performance? Think about the problem and summarize the ways you have tried (even those that did not work).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"><b>First Attempt</b></div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">For the first attempt, 33 features were used for the training set. The classifiers selected to perform the predictions were 'Random Forest' and 'K-Nearest Neighbor' with both models using the default parameters. In the performance evaluation, an accuracy of 98% was obtained for both models in the validation set.  While in the training set it reached 99% for RF and 98% for KNN. The decrease in accuracy achieved by RF in the validation set suggests a slight overfitting, however, it is a tolerable value that assumes that the model still performs well on the test set.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">According to the classification report, for almost all attempts the values of precision, recall and f1-scores reached high percentages, around 98.9%.  Except for the 'Impact' and 'Harmless' intents when applying RF as a model, the results obtained for these classes were quite lower compared to those obtained for the other classes, both in the training set and in the validation set, where 0% was obtained in each metric. The model presented a very poor performance when trying to classify these two classes.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">As for tuning techniques, K-fold cross-correlation and Grid Search were applied to see if the results obtained previously could be improved. However, when looking at the results obtained with 5 folds, the performance decreased considerably reaching an average of 77% accuracy. Suggests that the model may be in overfitting. </div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">Therefore, for the next attempt we consider a reduction of the dimensionality of the data set to improve generalization.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"><b>Second attempt</b></div><br>\n",
    "<div style=\"text-align: justify\">For the second attempt, the number of features was reduced to only 12 for the training set.  The classifiers used in the previous attempt, RF and KNN, were kept for this evaluation with both using their default parameters. The results obtained in the performance evaluation, the accuracy did not change from the previous attempt, reaching 98% for the RF case and 98% for the KNN for both the training and validation sets, indicating that the models continue to make good predictions for the classes.<br>\n",
    "<br>\n",
    "<div style=\"text-align: justify\">If we take a look at the classification report for almost all attempts, the values for accuracy, recall and f1-scores reached high percentages, around 98%. Nevertheless, the performance of the 'Impact' class was very poor in both models for the classification obtained in the validation data, with 0% for every metric (precision, recall and f1-score), indicating that the number of features selected to train both models was not sufficient to be able to correctly classify this class.<br><br>\n",
    "<div style=\"text-align: justify\">In both training and validation reports, the F1 score obtained for the classes \"Impact\" and \"Harmless\" was very low compared to the other classes. It seems that for both models, these two classes are the most difficult to classify correctly. This could be due to the fact that, the number of data selected during the splitting of the training set did not cover enough samples of these two classes, as they are the least sampled attempts of the whole dataset. </div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">After conducting multiple tests and adjusting the number of features in the training set, the results showed similar levels of accuracy. However, upon further analysis of the classification report parameters, it was found that the highest metrics for accuracy, recall, and F1-score were achieved with a number of features greater than 22. The F1-score parameter indicated that the models performed better in classifying samples belonging to the 'Impact' and 'Harmless' classes, which were more difficult to detect in almost all the tests. </div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
